# Chapter 1: Inference

From the following exercises, we obtain some remarks:  
1. Normal distribution:  
- The average of a random sample is a random variable.  
- The distribution of a random variable is the distribution of the list of values obtained if 
we repeated the experiment an infinite number of times. Increasing the number of times we repeat the 
experiment does not change drastically the distribution, but it allows us to better approximate it.  
- Increasing the sample size changes the random variable and thus its distribution.  
For eg: The larger the sample size, the smaller the spread of the distribution of mean(pop)-mean(sample).  

2. Populations, samples and estimates:  

3. Central Limit Theorem and t-distribution:  
- Use `popsd()` and `popvar()` of `rafalib` package to compute population standard deviation and variance.  
- Use `sd()` and `var()` to compute sample standard deviation and variance (with N-1 in the denominator).  
- When the distribution of the population values is approximately normal, as it is for the weights, 
the t-distribution provides a better approximation. The t-distribution has larger tails up until 30 
degrees of freedom, at which point it is practically the same as the normal distribution.  

4. Central Limit Theorem in practice:  
- The variance of binary data is `p * (1-p)`, where p is the success probability. For quantitative data, 
we need to estimate the population standard deviation.  
- With very low probabilities, we need larger sample sizes for the CLT to “kick in”.  
- The CLT tells us that the distribution of `Xbar` is approximately normal with mean **µX** and 
standard deviation σx/√n, where σx is the population standard deviation and n is the sample size. 
**Ideally, Xbar is obtained after infinite number of sampling of size n from the population.** In
practice, we only sample Xbar once. The bigger the sample size n, the smaller the SD of Xbar.
- `Z= √n*(Xbar-µX)/σx` follows a normal distribution with mean 0 and standard deviation 1. Important, 
both Z and Xbar are random variables and follow a distribution. 


## Normal Distribution

For these exercises, we will be using the following dataset:

```{r setup, include=TRUE}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename <- file.path("data", basename(url))
download(url, destfile=filename)
x <- unlist( read.csv(filename) )
write.csv(x, "data/femaleControlsPopulation.csv")
```

Here x represents the weights for the entire population.

1. What is the average of these weights?
```{r}
mean(x)
```

2. After setting the seed at 1, set.seed(1) take a random sample of size 5. What is the 
absolute value (use abs) of the difference between the average of the sample and the 
average of all the values?

```{r}
set.seed(1)
sample <- sample(x, 5)
abs(mean(sample) - mean(x))
```

3. After setting the seed at 5, set.seed(5) take a random sample of size 5. What is the 
absolute value of the difference between the average of the sample and the average of 
all the values?

```{r}
set.seed(5)
sample <- sample(x, 5)
abs(mean(sample) - mean(x))
```

4. Why are the answers from 2 and 3 different?  
**Answer:** C. Because the average of the samples is a random variable.

5. Set the seed at 1, then using a for-loop take a random sample of 5 mice 1,000 times. 
Save these averages. What percent of these 1,000 averages are more than 1 ounce away 
from the average of x ?

```{r}
set.seed(1)
n <- 1000
null <- vector("numeric",n)
for (i in 1:n) {
    sample <- sample(x, 5)
    null[i] <- mean(sample)- mean(x)
}
mean(null > 1)
hist(null)
```

6. We are now going to increase the number of times we redo the sample from 1,000 to 10,000.
Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save
these averages. What percent of these 10,000 averages are more than 1 ounce away from the
average of x ?

```{r}
set.seed(1)
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
    sample <- sample(x, 5)
    null[i] <- mean(sample)- mean(x)
}
mean(null > 1)
hist(null)
```

7. Note that the answers to 5 and 6 barely changed. This is expected. The way we think 
about the random value distributions is as the distribution of the list of values 
obtained if we repeated the experiment an infinite number of times. On a computer, we 
can’t perform an infinite number of iterations so instead, for our examples, we consider 1,000 
to be large enough, thus 10,000 is as well. Now if instead we change the sample size, then we change
the random variable and thus its distribution.

Set the seed at 1, then using a for-loop take a random sample of 50 mice 1,000 times. 
Save these averages. What percent of these 1,000 averages are more than 1 ounce away 
from the average of x ?

```{r}
set.seed(1)
n <- 1000
null <- vector("numeric",n)
for (i in 1:n) {
    sample <- sample(x, 50)
    null[i] <- mean(sample)- mean(x)
}
mean(null > 1)

hist(null)
```

8. Use a histogram to “look” at the distribution of averages we get with a sample size of 5 and
a sample size of 50. How would you say they differ?  
**Answer:** B. They both look roughly normal, but with a sample size of 50 the spread is smaller.
When we have more samples, the difference between the mean of the sample and the mean of the 
population is smaller, i.e. we could infer the population mean better with more samples.

9. For the last set of averages, the ones obtained from a sample size of 50, what percent are
between 23 and 25?

```{r}
set.seed(1)
n <- 1000
avg <- vector("numeric",n)
for (i in 1:n) {
    sample <- sample(x, 50)
    avg[i] <- mean(sample)
}
mean(avg > 23 & avg < 25)
hist(avg)
```


10. Now ask the same question of a normal distribution with average 23.9 and standard deviation 0.43.
`pnorm` computes the cumulative distribution function (CDF) of the normal distribution. 
It calculates the probability that a value is less than or equal to a given point.

```{r}
#calculating the probability that values are greater than 23 and 25 separately, rather than the probability that values are between 23 and 25.
# 1 - pnorm(q= c(23,25), mean = 23.9, sd= 0.43) 
#correct code
pnorm(q= 25, mean = 23.9, sd= 0.43) - pnorm(q= 23, mean = 23.9, sd= 0.43)
```

The answer to 9 and 10 were very similar. This is because we can approximate the distribution of
the sample average with a normal distribution. We will learn more about the reason for this next.

## Populations, Samples and Estimates

```{r}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- file.path("data", basename(url))
download(url, destfile=filename)
dat <- read.csv(filename)
dat <- na.omit(dat)
head(dat)
```

1. Use dplyr to create a vector x with the body weight of all males on the control (chow) diet.
What is this population’s average?

```{r , warning=FALSE}
library(dplyr)
m_chow <- dat %>%
    filter(Sex == "M", Diet == "chow") %>%
    select(Bodyweight) %>%
    unlist()
mean(m_chow)
```

2. Now use the rafalib package and use the popsd function to compute the population
standard deviation.

```{r}
#install.packages("rafalib")
library(rafalib)
popsd(m_chow)
```

3. Set the seed at 1. Take a random sample X of size 25 from x. What is the sample average?

```{r}
set.seed(1)
sample_m_chow <- sample(m_chow,25)
mean(sample_m_chow)
```

4. Use dplyr to create a vector y with the body weight of all males on the high fat (hf) diet.
What is this population’s average?

```{r}
m_hf <- dat %>%
    filter(Sex == "M", Diet == "hf") %>%
    select(Bodyweight) %>%
    unlist()
mean(m_hf)
```    

5. Now use the rafalib package and use the popsd function to compute the population
standard deviation.

```{r}
popsd(m_hf)
```

6. Set the seed at 1. Take a random sample Y of size 25 from y. What is the sample average?

```{r}
set.seed(1)
sample_m_hf <- sample(m_hf,25)
mean(sample_m_hf)
```

7. What is the difference in absolute value between mean(m_hf)-mean(m_chow) and mean(sample_m_chow)-mean(sample_m_hf)?

```{r}
#abs(abs((mean(y) - mean(x))) - abs((mean(sample_y) - mean(sample_x))))
abs((mean(m_hf) - mean(m_chow)) - (mean(sample_m_hf) - mean(sample_m_chow)))
```

8. Repeat the above for females. Make sure to set the seed to 1 before each sample call. What
is the difference in absolute value between mean(y)-mean(x) and mean(sample_y)-mean(sample_x)?

```{r}
#Population average and standard deviation for females mouse on chow diet
library(dplyr)
f_chow <- dat %>%
    filter(Sex == "F", Diet == "chow") %>%
    select(Bodyweight) %>%
    unlist()
mean(f_chow)
popsd(f_chow)

#Sample average for 25 females on chow diet
set.seed(1)
sample_f_chow <- sample(f_chow,25)
mean(sample_f_chow)
```

```{r}
#Population average and standard deviation for females mouse on hf diet
library(dplyr)
f_hf <- dat %>%
    filter(Sex == "F", Diet == "hf") %>%
    select(Bodyweight) %>%
    unlist()
mean(f_hf)
popsd(f_hf)

#Sample average for 25 females on hf diet
set.seed(1)
sample_f_hf <- sample(f_hf,25)
mean(sample_f_hf)
```

```{r}
abs((mean(f_hf) - mean(f_chow)) - (mean(sample_f_hf) - mean(sample_f_chow)))
```

9. For the females, our sample estimates were closer to the population difference than with
males. What is a possible explanation for this?  

**Answer:**  
- With seed=1, males closer (1.40 < 1.75). Females *typically* closer due to **lower population SDs**.  
- Typical correct answer: A. The population variance of the females is smaller than that of the males; thus, the
sample variable has less variability.

## Central Limit Theorem and t-distribution

When the sample size is large, the average of a random sample follows a normal distribution 
centered at the population average µ and with standard deviation equal to the population 
standard deviation σ, divided by the square root of the sample size N. We refer to the 
standard deviation of the distribution of a random variable as the random variable’s 
**standard error**.

```{r}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
filename <- file.path("data", basename(url))
download(url, destfile=filename)
dat <- na.omit(read.csv(filename))
```

1. If a list of numbers has a distribution that is well approximated by the normal distribution,
what proportion of these numbers are within one standard deviation away from the list’s
average?

```{r}
pnorm(q=1, mean=0, sd=1) - pnorm(q=-1, mean=0, sd=1)
``` 
For the normal distribution: ~68% of values are within 1 SD

2. What proportion of these numbers are within two standard deviations away from the list’s
average?

```{r}
pnorm(q=2, mean=0, sd=1) - pnorm(q=-2, mean=0, sd=1)
``` 
For the normal distribution: ~95% of values are within 2 SDs

3. What proportion of these numbers are within three standard deviations away from the list’s
average?

```{r}
pnorm(q=3, mean=0, sd=1) - pnorm(q=-3, mean=0, sd=1)
``` 
For the normal distribution: ~99.7% of values are within 3 SDs

4. Define y to be the weights of males on the control diet. What proportion of the mice are
within one standard deviation away from the average weight (remember to use popsd for
the population sd)?

```{r}
y <- dat %>%
    filter(Sex == "M", Diet == "chow") %>%
    select(Bodyweight) %>%
    unlist()
mean(y)
popsd(y)
hist(y)

pnorm(q= mean(y)+ popsd(y), mean=mean(y), sd= popsd(y)) - pnorm(q= mean(y)- popsd(y), mean=mean(y), sd= popsd(y))
```
For y that approximately follows a normal distribution: ~68% of values are within 1 SD

5. What proportion of these numbers are within two standard deviations away from the list’s
average?

```{r}
pnorm(q= mean(y)+ 2*popsd(y), mean=mean(y), sd= popsd(y)) - pnorm(q= mean(y)- 2*popsd(y), mean=mean(y), sd= popsd(y))
```
For y that approximately follows a normal distribution: ~95% of values are within 2 SDs

6. What proportion of these numbers are within three standard deviations away from the list’s
average?
```{r}
pnorm(q= mean(y)+ 3*popsd(y), mean=mean(y), sd= popsd(y)) - pnorm(q= mean(y)- 3*popsd(y), mean=mean(y), sd= popsd(y))
```
For y that approximately follows a normal distribution: ~99.7% of values are within 3 SDs

7. Note that the numbers for the normal distribution and our weights are relatively close. Also,
notice that we are indirectly comparing quantiles of the normal distribution to quantiles
of the mouse weight distribution. We can actually compare all quantiles using a qqplot.

```{r}
qqnorm(y)
qqline(y)
```

**Answer:** C) The mouse weights are well approximated by the normal distribution, although
the larger values (right tail) are larger than predicted by the normal. This is consistent
with the differences seen between question 3 and 6. 

8. Create the above qq-plot for the four populations: male/females on each of the two diets. What
is the best explanation for all these being well approximated by the normal distribution?

```{r}
mypar(2,2)
#Male chow
y_m_chow <- dat %>%
    filter(Sex == "M", Diet == "chow") %>%
    select(Bodyweight) %>%
    unlist()
qqnorm(y_m_chow)
qqline(y_m_chow)        

#Male hf
y_m_hf <- dat %>%
    filter(Sex == "M", Diet == "hf") %>%
    select(Bodyweight) %>%
    unlist()
qqnorm(y_m_hf)
qqline(y_m_hf)      

#Female chow
y_f_chow <- dat %>%
    filter(Sex == "F", Diet == "chow") %>%
    select(Bodyweight) %>%
    unlist()
qqnorm(y_f_chow)
qqline(y_f_chow)    

#Female hf
y_f_hf <- dat %>%
    filter(Sex == "F", Diet == "hf") %>%
    select(Bodyweight) %>%
    unlist()
qqnorm(y_f_hf)
qqline(y_f_hf)
```

**Answer:** A) The CLT tells us that sample averages are approximately normal when the sample size
is large.

9. Here we are going to use the function `replicate` to learn about the distribution of random
variables. All the above exercises relate to the normal distribution as an approximation of the
distribution of a fixed list of numbers or a population. We have not yet discussed probability
in these exercises. If the distribution of a list of numbers is approximately normal, then if
we pick a number at random from this distribution, it will follow a normal distribution.  

However, it is important to remember that stating that some quantity has a distribution does
not necessarily imply this quantity is random. Also, keep in mind that this is not related to
the central limit theorem. The central limit applies to averages of random variables. Let’s
explore this concept.

We will now take a sample of size 25 from the population of males on the chow diet. The
average of this sample is our random variable. We will use the replicate to observe 10,000
realizations of this random variable. Set the seed at 1, generate these 10,000 averages. Make
a histogram and qq-plot of these 10,000 numbers against the normal distribution.
We can see that, as predicted by the CLT, the distribution of the random variable is very
well approximated by the normal distribution.


```{r }
set.seed(1)
y <- filter(dat, Sex=="M" & Diet=="chow") %>% select(Bodyweight) %>% unlist
avgs <- replicate(10000, mean(sample(y, 25)))
mypar(1,2)
hist(avgs)
qqnorm(avgs)
qqline(avgs)
```

What is the average of the distribution of the sample average?

```{r}
mean(avgs)
```

10. What is the standard deviation of the distribution of sample averages?

```{r}
sd(avgs)
#sd() and var() computes sample SD with N-1 in the denominator:
samplevar <- sum((avgs-mean(avgs))^2)/(length(avgs)-1)
samplevar == var(avgs)
```

11. According to the CLT, the answer to exercise 9 should be the same as mean(y). You should
be able to confirm that these two numbers are very close. Which of the following does the
CLT tell us should be close to your answer to exercise 10?

```{r}
popsd(y)/sqrt(25)
```

12. In practice we do not know σ (popsd(y)) which is why we can’t use the CLT directly. This
is because we see a sample and not the entire distribution. We also can’t use popsd(avgs)
because to construct averages, we have to take 10,000 samples and this is never practical. We
usually just get one sample. Instead we have to estimate popsd(y). As described, what we
use is the sample standard deviation. Set the seed at 1, using the replicate function, create
10,000 samples of 25 and now, instead of the sample average, keep the standard deviation.
Look at the distribution of the sample standard deviations. It is a random variable. The real
population SD is about 4.5. What proportion of the sample SDs are below 3.5?

```{r}
set.seed(1)
y <- dat %>% filter(Sex=="M" & Diet=="chow") %>% select(Bodyweight) %>% unlist
sds <- replicate(10000, sd(sample(y, 25)))
hist(sds)
mean(sds < 3.5)
```

13. What the answer to question 12 reveals is that the denominator of the t-test is a random
variable. By decreasing the sample size, you can see how this variability can increase.
It therefore adds variability. The smaller the sample size, the more variability is added.
The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, 
the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference
between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of
x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which
of the following is true?

```{r}
x <- seq(0.0001,0.9999,len=300)
mypar(3,2)
plot(x, qt(x, df=3), type="l", main="df=3")
plot(x, qt(x, df=10), type="l", main="df=10")
plot(x, qt(x, df=30), type="l", main="df=30")
plot(x, qt(x, df=100), type="l", main="df=100")
plot(x, qnorm(x), type="l", main="Normal")
lines(x, qnorm(x), col="red")
```

**Answer:**  C) The t-distribution has larger tails up until 30 degrees of freedom, at which point it
is practically the same as the normal distribution.

## Central Limit Theorem in Practice

```{r}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename <- file.path("data","femaleMiceWeights.csv")
if(!file.exists(filename)) download(url, destfile=filename)
dat <- read.csv(filename)
```

1. The CLT is a result from probability theory. Much of probability theory was originally
inspired by gambling. This theory is still used in practice by casinos. For example, they can
estimate how many people need to play slots for there to be a 99.9999% probability of earning
enough money to cover expenses. Let’s try a simple example related to gambling.  

Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This
is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and
the proportion we are interested in can be expressed as an average: mean(x==6). Because the
die rolls are independent, the CLT applies.  

We want to roll n dice 10,000 times and keep these proportions. This random variable
(proportion of 6s) has mean `p=1/6` and variance `p * (1-p)/n`. So according to CLT `z=(mean(x==6) - p) / sqrt(p*(1-p)/n)` 
should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the 
simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says 
it should be about 0.05).

Each die role is independent with P(6) = 1/6 exactly.

```{r}
#Role 100 die once and keep the proportion of 6s (one sampling)
set.seed(1)
n<- 100
x= sample(1:6, n, replace=TRUE)
hist(x)
prob_6 <- mean(x==6)
prob_6
```

As number of dices (n) grows to 100, the proportion of times we see a 6 with this sample approximate 
the number of times we see a 6 with a fair dice where die rolls are independent, which is ~ `P(6)= 1/6`.

```{r}
#Roll 100 die 10,000 times and keep the proportions of 6s (10k-times sampling)
set.seed(1)
n <- 100
t <- 10000
p <- 1/6
prop_6_10k <- replicate(t, {
    x <- sample(1:6, n, replace = TRUE)
    prob_6_10k <- mean(x == 6) 
    return(prob_6_10k) 
})

hist(prop_6_10k)
mean(prop_6_10k)- p

var_6_10k <- popvar(prop_6_10k)
var_6_10k- (p*(1-p)/n)
```

If we repeat this experiment 10000 times, this random variable (proportion of 6s) has mean ~ `p=1/6` 
and variance ~ `p*(1-p)/n`.  

According to CLT `z=(prop_6_10k - p) / sqrt(p*(1-p)/n)` should be normal with mean 0 and SD 1. 

```{r}
z <- (prop_6_10k - p) / sqrt(p*(1-p)/n) #z should be normal with mean 0 and SD 1 (CLT)
hist(z)
#The proportion of times z was larger than 2 in absolute value
mean(abs(z) > 2)
pnorm_2 <- 1-pnorm(q=2, mean=0, sd=1)+ pnorm(q=-2, mean=0, sd=1)
```

The proportion of times z was larger than 2 in absolute value is `r mean(abs(z) > 2)`, and CLT 
says it should be about 0.05.


2. For the last simulation you can make a qqplot to confirm the normal approximation. 
Now, the CLT is an *asympototic* result, meaning it is closer and closer to being a perfect
approximation as the sample size increases. In practice, however, we need to decide if it is
appropriate for actual sample sizes. Is 10 enough? 15? 30?
In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the
success probability also affects the appropriateness of the CLT. With very low probabilities,
we need larger sample sizes for the CLT to “kick in”.
Run the simulation from exercise 1, but for different values of p and n. For which of the
following is the normal approximation best?

```{r}
qqnorm(z)
qqline(z)
```

```{r}
set.seed(1)
n <- 5
t <- 10000
p <- 0.5
z <- replicate(t, {
    x <- sample(1:6, n, replace = TRUE)
    prob_6_10k <- mean(x == 6) 
    (prop_6_10k - p) / sqrt(p*(1-p)/n)
})
hist(z)

set.seed(1)
n <- 30
t <- 10000
p <- 0.5
z <- replicate(t, {
    x <- sample(1:6, n, replace = TRUE)
    prob_6_10k <- mean(x == 6) 
    (prop_6_10k - p) / sqrt(p*(1-p)/n)
})
hist(z)

set.seed(1)
n <- 30
t <- 10000
p <- 0.01
z <- replicate(t, {
    x <- sample(1:6, n, replace = TRUE)
    prob_6_10k <- mean(x == 6) 
    (prop_6_10k - p) / sqrt(p*(1-p)/n)
})
hist(z)

set.seed(1)
n <- 100
t <- 10000
p <- 0.01
z <- replicate(t, {
    x <- sample(1:6, n, replace = TRUE)
    prob_6_10k <- mean(x == 6) 
    (prop_6_10k - p) / sqrt(p*(1-p)/n)
})
hist(z)
```

**Answer** D) p=0.01 and n=100

3. As we have already seen, the CLT also applies to averages of quantitative data. A major
difference with binary data, for which we know the variance is `p * (1−p)`, is that with
quantitative data we need to estimate the population standard deviation.

In several previous exercises we have illustrated statistical concepts with the unrealistic
situation of having access to the entire population. In practice, we do not have access to
entire populations. Instead, we obtain one random sample and need to reach conclusions
analyzing that data. dat is an example of a typical simple dataset representing just one
sample. We have 12 measurements for each of two populations:

```{r }
X <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
Y <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist
```

We think of X as a random sample from the population of all mice in the control diet and Y as a random 
sample from the population of all mice in the high fat diet.

Define the parameter µx as the average of the control population. We estimate this parameter with the 
sample average. What is the sample average?

```{r} 
mean(X)
```

4. We don’t know µX, but want to use Xbar to understand µX. Which of the following uses CLT to 
understand how well Xbar approximates µX?  

A) Xbar follows a normal distribution with mean 0 and standard deviation 1.  
--> **Wrong** not Xbar= 0, but **standardized difference** `Z= (Xbar-µX)/(σx/√n)` have means= 0.  

B) µX follows a normal distribution with mean Xbar and standard deviation σx√12 where σx is the population standard deviation.  
--> **Wrong**: **µX is a fixed parameter**, not a random variable, thus it does not have a distribution.  

C) Xbar follows a normal distribution with mean µX and standard deviation σx where σx is the population standard deviation.  
--> **Wrong**: The standard deviation of the distribution of Xbar is not σx, but σx/√n.  

D) Xbar follows a normal distribution with mean µX and standard deviation σx/√12 where σx is the population standard deviation.  
--> **Correct**: The CLT tells us that the distribution of Xbar is approximately normal with mean **µX** and standard deviation σx/√n, 
where σx is the population standard deviation and n is the sample size (12 in this case).  

5. The result above tells us the distribution of the following random variable: `Z= √12*(Xbar-µX)/σx`.
What does the CLT tell us is the mean of Z (you don’t need code)?  
--> **Answer:** 0. The CLT tells us that Z follows a normal distribution with mean 0 and standard deviation 1.

6. The result of 4 and 5 tell us that we know the **distribution of the difference between our estimate and what we want to estimate, but don’t know**. 
However, the equation involves the population standard deviation σX, which we don’t know. Given what we discussed,
what is your estimate of σx?  
--> **Answer:** σx is estimated as the **standard deviation of the sample sx**, based on CLT.

7. Use the CLT to approximate the probability that our estimate Xbar is off by more than 5.21 ounces from µX.

--> **Answer:** this means that we want to calculate the probability that the absolute value of the 
difference between Xbar and µX is greater than 5.21 ounces. We want to find `P(|Z| > (5.21 / (σx/√n)))` 
or `P(|Z| > z0)`. This can be calculated using the cumulative distribution function of the normal distribution.

```{r}
#Use sd() instead of popsd() to estimate σx, because we only have one sample and thus we need to use the sample standard deviation with N-1 in the denominator.
z0 <- 5.21 / (sd(X) / sqrt(length(X)))
z0 <- sqrt(12) * 5.21/ sd(X) #same as above
(1- pnorm(q= z0, mean = 0, sd= 1)) + pnorm(q= -z0, mean = 0, sd= 1)
```

This prob is very small, which means that it is very unlikely that our estimate Xbar is off by more than 5.21 ounces from µX, 
according to the CLT approximation. This means **our sample average is a good estimate of the population mean µX.**

8 (theory on page 35). Now we introduce the concept of a null hypothesis. We don’t know µx nor µy. We want to
quantify what the data say about the possibility that the diet has no effect: µx = µy. If we
use CLT, then we approximate the distribution of `Xbar` as normal with mean µX and standard
deviation σX and the distribution of `Ybar` as normal with mean µY and standard deviation
σY. This implies that the difference `Ybar−Xbar` has mean 0 (there is no difference). We described that the 
standard deviation of this statistic (the standard error) is SE(Xbar−Ybar) = sqrt(σx^2/n + σy^2/n) and that
we estimate the population standard deviations σx and σy with the sample estimates. What is the 
estimate of SE(Xbar−Ybar) = sqrt(σx^2/n + σy^2/n)?

```{r}
sd_X <- sd(X)
sd_Y <- sd(Y)
SE <- sqrt(sd_X^2/length(X) + sd_Y^2/length(Y))
SE
```

9. So now we can compute `Ybar−Xbar` as well as an estimate of this standard error and construct
a t-statistic. What is this t-statistic?

--> **Answer:** `t = (Ybar−Xbar) / SE(Xbar−Ybar)`

10. If we apply the CLT, what is the distribution of this t-statistic?  

A) Normal with mean 0 and standard deviation 1.
--> Correct. The CLT tells us that **under the null hypothesis that there is no difference between the population averages**,
the distribution of the difference between the sample means (Ybar−Xbar) is approximately normal with mean 
0 and standard deviation SE(Xbar−Ybar). Thus, when we standardize this difference by dividing by its 
estimated standard error, we get a t-statistic that follows a normal distribution with mean 0 and 
standard deviation 1 under the null hypothesis.

11. Now we are ready to compute a p-value using the CLT. What is the probability of observing
a quantity as large as what we computed in 10, when the null distribution is true?

--> **Answer:** We can compute the p-value using the cumulative distribution function of the normal 
distribution.

```{r}
t_statistic <- (mean(Y) - mean(X)) / SE
t_statistic

p_value <- 2 * (1 - pt(abs(t_statistic), df=22)) #this use t-distribution with df=22 (n1+n2-2) to compute the p-value, 
#because we are estimating the population standard deviation with the sample standard deviation, and thus we need to account 
#for this additional variability by using the t-distribution instead of the normal distribution.
p_value

p_value_clt <- 2 * (1 - pnorm(abs(t_statistic)))
p_value_clt <- (1 - pnorm(abs(t_statistic)))+ pnorm(-abs(t_statistic)) #same as above
p_value_clt
```

12. CLT provides an approximation for cases in which the sample size is large. In practice, we
can’t check the assumption because we only get to see 1 outcome (which you computed
above). As a result, if this approximation is off, so is our p-value. As described earlier, there
is another approach that does not require a large sample size, but rather that the distribution
of the population is approximately normal. We don’t get to see this distribution so it is
again an assumption, although we can look at the distribution of the sample with qqnorm(X)
and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows
t-distribution. What is the p-value under the t-distribution approximation? Hint: use the
t.test function.  

--> Rephrase: CLT Problem: Works for large n (like n=30+), but here n=12 is small → p-value might be inaccurate.
Better Alternative (no large n needed):  
- Assume X and Y data are approximately normal (check with qqnorm(X) and qqnorm(Y) plots).  
- Then t-statistic exactly follows t-distribution with df=22 (not just CLT approximation).

```{r}
par(mfrow=c(1,2))
qqnorm(X)
qqline(X)

qqnorm(Y)
qqline(Y)
```

```{r}
t_test_result <- t.test(X, Y, var.equal = TRUE) 
#var.equal=TRUE assumes that the population variances are equal, which is a common assumption in 
# two-sample t-tests. If we do not want to assume equal variances, we can set var.equal=FALSE, 
# which will perform Welch's t-test that does not assume equal variances.
t_test_result
```

13. With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. 
They can’t both be right. What best describes the difference?  

A) A sample size of 12 is not large enough, so we have to use the t-distribution approximation.  
--> **Correct but not complete**: With a sample size of 12, the CLT approximation may not be accurate, and thus the p-value obtained 
from the CLT may be misleading. **t works regardless of n size if normal**.  

B) These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation 
of the standard error and thus, under the null, large values are more probable under the null distribution.  
--> **Correct**: The t-distribution accounts for the additional variability introduced by estimating the population 
standard deviation with the sample standard deviation. **t has fatter tails because sample SE varies**.  

C) The population data is probably not normally distributed so the t-distribution approximation is wrong.
--> **Wrong**: We can check the normality assumption with the qqnorm plots. If the data points in the qqnorm plots 
approximately follow a straight line, then the normality assumption is reasonable, and thus the t-distribution 
approximation is appropriate.  

D) Neither assumption is useful. Both are wrong.
--> **Wrong**: The t-distribution approximation is useful when the sample size is small and the normality assumption is 
reasonable, which seems to be the case here based on the qqnorm plots.  



